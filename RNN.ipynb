{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/ZiyangS/AI_5010_4010_Deep_Learning/blob/main/RNN.ipynb)\n",
    "\n",
    "# Recurrent Neural Network (RNN)\n",
    "Recurrent Neural Networks (RNNs) are designed to capture the sequential structure in data. Unlike feed-forward networks—which treat each input as independent—RNNs model dependencies across time. This is important for tasks like language modeling, where predicting the next word depends on the words that came before it."
   ],
   "metadata": {
    "id": "LjI19B6Dm7PB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vanilla RNN\n",
    "\n",
    "The input $x$ will be a sequence of words, and each $x_t$ is a single word. Because matrix multiplication requires vector inputs, we cannot directly use a word index (such as 36) as the input. Instead, we represent each word as a one-hot vector whose length equals the vocabulary size. For example, a word with index 36 would be represented as a vector where the 36-th position is 1 and all other positions are 0.\n"
   ],
   "metadata": {
    "id": "pXAuUATtnfnl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(777)"
   ],
   "metadata": {
    "id": "FeFfHWTVn8C7",
    "ExecuteTime": {
     "end_time": "2025-11-20T05:54:42.116403Z",
     "start_time": "2025-11-20T05:54:41.516260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1075794d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:54:42.120747Z",
     "start_time": "2025-11-20T05:54:42.119190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# suppose we have a one-hot encoding for each character in 'hello‘ and\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# the sequence length for the word 'hello' is 5\n",
    "seq_len = 5\n",
    "\n",
    "# our input shape should be (batch, seq_len, feature_size) when batch_first=True.\n",
    "# Even if we have only one sequence, PyTorch still requires a batch dimension.\n",
    "# Thus we reshape \"hello\" from shape (5, 4) to (1, 5, 4), where batch_size=1.\n",
    "inputs = torch.Tensor([h, e, l, l, o])\n",
    "inputs = inputs.view(1, 5, -1)\n",
    "print(inputs.shape)"
   ],
   "metadata": {
    "id": "yU-R8damo-Ht",
    "outputId": "7c61d500-4638-4156-c431-c0cf9da17c2b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2025-11-20T05:54:42.127850Z",
     "start_time": "2025-11-20T05:54:42.125788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 4])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:54:42.138412Z",
     "start_time": "2025-11-20T05:54:42.134386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We specify a single RNN cell with the property of input_dim (4) -> output_dim (2)\n",
    "rnn_cell = nn.RNN(input_size=4, hidden_size=2, batch_first=True)\n",
    "\n",
    "# Initialize the hidden state: Shape: (batch, num_layers, hidden_size)\n",
    "hidden = torch.zeros(1, 1, 2)\n",
    "\n",
    "# Run the RNN: 'out' contains the hidden state at each time step\n",
    "# 'hidden' is the final hidden state of the sequence.\n",
    "out, hidden = rnn_cell(inputs, hidden)\n",
    "print('sequence input size', inputs.size())\n",
    "print('out size', out.size())\n",
    "print('final hidden state size', hidden.size())\n",
    "\n",
    "# 'out' contains the hidden state at every time step, while 'hidden' stores only the last one.\n",
    "# The final step of 'out' should match 'hidden'.\n",
    "print('\\ncomparing rnn cell output:')\n",
    "print(out[:, -1, :])\n",
    "print(hidden[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence input size torch.Size([1, 5, 4])\n",
      "out size torch.Size([1, 5, 2])\n",
      "final hidden state size torch.Size([1, 1, 2])\n",
      "\n",
      "comparing rnn cell output:\n",
      "tensor([[-0.7762,  0.8319]], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.7762,  0.8319]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "# create an index to character mapping\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "# Teach hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]    # hihell\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0]]]  # l 3\n",
    "\n",
    "x_one_hot = np.array(x_one_hot)\n",
    "y_data = np.array([1, 0, 2, 3, 3, 4])  # ihello\n",
    "\n",
    "# As we have one batch of samples, we will change them to variables only once\n",
    "inputs = torch.Tensor(x_one_hot)\n",
    "labels = torch.LongTensor(y_data)\n",
    "\n",
    "# hyperparameters\n",
    "seq_len = 6      # |hihell| == 6, equivalent to time step\n",
    "input_size = 5   # one-hot size\n",
    "batch_size = 1   # one sentence per batch\n",
    "num_layers = 1   # one-layer rnn\n",
    "num_classes = 5  # predicting 5 distinct character\n",
    "hidden_size = 4  # output from the RNN"
   ],
   "metadata": {
    "id": "kz0Phf2Go-ow",
    "outputId": "841e078f-60bf-49a2-a2bf-c4fa7aaba596",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 829
    },
    "ExecuteTime": {
     "end_time": "2025-11-20T05:54:42.143044Z",
     "start_time": "2025-11-20T05:54:42.141047Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model will be a RNN followed by a linear layer,\n",
    "    i.e. a fully-connected layer\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, num_classes, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # assuming batch_first = True for RNN cells\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        x = x.view(batch_size, self.seq_len, self.input_size)\n",
    "\n",
    "        # 'rnn_out' contains the hidden state for every time step. Shape: [batch_size, seq_len, hidden_size].\n",
    "        # ’linear_out‘ is the outputs using all hidden states for each time step individually. Shape: [batch_size, seq_len, num_classes].\n",
    "        rnn_out, _ = self.rnn(x, hidden)\n",
    "        linear_out = self.linear(rnn_out.view(-1, hidden_size))\n",
    "        return linear_out\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize hidden cell states, assuming batch_first = True for RNN cells\n",
    "        \"\"\"\n",
    "        return torch.zeros(batch_size, self.num_layers, self.hidden_size)\n"
   ],
   "metadata": {
    "id": "8pwJoRE9pER9",
    "outputId": "28214b1d-7c18-41cc-9fc5-9e1cb987c901",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2025-11-20T05:54:42.149170Z",
     "start_time": "2025-11-20T05:54:42.146051Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "# Set loss, optimizer and the RNN model\n",
    "rnn = RNN(seq_len, num_classes, input_size, hidden_size, num_layers)\n",
    "print('network architecture:\\n', rnn)"
   ],
   "metadata": {
    "id": "5FDwWFKSouTV",
    "outputId": "db3080c6-14fa-42a8-f656-61211c4c2de1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2025-11-20T05:54:42.153917Z",
     "start_time": "2025-11-20T05:54:42.152075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network architecture:\n",
      " RNN(\n",
      "  (rnn): RNN(5, 4, batch_first=True)\n",
      "  (linear): Linear(in_features=4, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "# train the model\n",
    "num_epochs = 15\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=0.1)\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = rnn(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # check the current predicted string, max gives the maximum value and its corresponding index\n",
    "    _, idx = outputs.max(dim = 1)\n",
    "    idx = idx.detach().numpy()\n",
    "    result_str = [idx2char[c] for c in idx]\n",
    "    print('epoch: {}, loss: {:1.3f}'.format(epoch, loss.item()))\n",
    "    print('Predicted string: ', ''.join(result_str))"
   ],
   "metadata": {
    "id": "ZlJIVrqfpN3M",
    "outputId": "1edb9506-b772-4f32-d3e2-c7941e37a88f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "ExecuteTime": {
     "end_time": "2025-11-20T05:54:42.540447Z",
     "start_time": "2025-11-20T05:54:42.158841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 1.658\n",
      "Predicted string:  llllol\n",
      "epoch: 2, loss: 1.500\n",
      "Predicted string:  llllll\n",
      "epoch: 3, loss: 1.387\n",
      "Predicted string:  llllll\n",
      "epoch: 4, loss: 1.260\n",
      "Predicted string:  ililll\n",
      "epoch: 5, loss: 1.152\n",
      "Predicted string:  ililll\n",
      "epoch: 6, loss: 1.047\n",
      "Predicted string:  ililll\n",
      "epoch: 7, loss: 0.926\n",
      "Predicted string:  ililll\n",
      "epoch: 8, loss: 0.808\n",
      "Predicted string:  ililll\n",
      "epoch: 9, loss: 0.698\n",
      "Predicted string:  ehello\n",
      "epoch: 10, loss: 0.601\n",
      "Predicted string:  ehello\n",
      "epoch: 11, loss: 0.522\n",
      "Predicted string:  ehello\n",
      "epoch: 12, loss: 0.455\n",
      "Predicted string:  ehello\n",
      "epoch: 13, loss: 0.397\n",
      "Predicted string:  ehello\n",
      "epoch: 14, loss: 0.348\n",
      "Predicted string:  ehello\n",
      "epoch: 15, loss: 0.312\n",
      "Predicted string:  ihello\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LSTM\n",
    "\n",
    "The following example uses an LSTM to generate part-of-speech (POS) tags. Its structure is similar to the RNN from the previous section, but here we add an embedding layer before the LSTM. Instead of representing each word with a one-hot vector—which is high-dimensional and ignores relationships between words—the embedding layer maps each word index to a dense vector that captures semantic similarity within the corpus."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:54:42.545664Z",
     "start_time": "2025-11-20T05:54:42.543975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small for this toy example\n",
    "EMBEDDING_SIZE = 6\n",
    "HIDDEN_SIZE = 6\n",
    "\n",
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "\n",
    "idx_to_tag = ['DET', 'NN', 'V']\n",
    "tag_to_idx = {'DET': 0, 'NN': 1, 'V': 2}\n",
    "\n",
    "word_to_idx = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "print(word_to_idx)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:54:42.552627Z",
     "start_time": "2025-11-20T05:54:42.550778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_sequence(seq, to_idx):\n",
    "    \"\"\"Convert sentence/sequence to torch Tensors\"\"\"\n",
    "    idxs = [to_idx[w] for w in seq]\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "seq = training_data[0][0]\n",
    "inputs = prepare_sequence(seq, word_to_idx)\n",
    "\n",
    "print(\"Original sequence: \", seq)\n",
    "print(\"Indexed tensor:    \", inputs.tolist())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sequence:  ['The', 'dog', 'ate', 'the', 'apple']\n",
      "Indexed tensor:     [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:54:42.558708Z",
     "start_time": "2025-11-20T05:54:42.556666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, vocab_size, tagset_size):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tagset_size = tagset_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size)\n",
    "        self.hidden2tag = nn.Linear(hidden_size, tagset_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        hidden = self._init_hidden()\n",
    "\n",
    "        # Reshape to (seq_len, batch_size=1, embedding_size) since this example uses batch size 1.\n",
    "        lstm_out, lstm_hidden = self.lstm(embed.view(len(x), 1, -1), hidden)\n",
    "        # Apply the linear layer to each time step to obtain tag scores.\n",
    "        output = self.hidden2tag(lstm_out.view(len(x), -1))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _init_hidden(self):\n",
    "        # the dimension semantics are [num_layers, batch_size, hidden_size]\n",
    "        return (torch.rand(1, 1, self.hidden_size),\n",
    "                torch.rand(1, 1, self.hidden_size))\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:54:42.567345Z",
     "start_time": "2025-11-20T05:54:42.565194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lstm = LSTMTagger(EMBEDDING_SIZE, HIDDEN_SIZE, len(word_to_idx), len(tag_to_idx))\n",
    "print('network architecture:\\n', lstm)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network architecture:\n",
      " LSTMTagger(\n",
      "  (embedding): Embedding(9, 6)\n",
      "  (lstm): LSTM(6, 6)\n",
      "  (hidden2tag): Linear(in_features=6, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:54:42.819400Z",
     "start_time": "2025-11-20T05:54:42.570500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(lstm.parameters(), lr=0.1)\n",
    "\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    for sentence, tags in training_data:\n",
    "        lstm.zero_grad()\n",
    "\n",
    "        sentence = prepare_sequence(sentence, word_to_idx)\n",
    "        target = prepare_sequence(tags, tag_to_idx)\n",
    "\n",
    "        output = lstm(sentence)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:54:42.825995Z",
     "start_time": "2025-11-20T05:54:42.823662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = prepare_sequence(training_data[0][0], word_to_idx)\n",
    "tag_scores = lstm(inputs)\n",
    "\n",
    "# validating that the sentence \"the dog ate the apple\".\n",
    "# the correct tag should be DET NOUN VERB DET NOUN\n",
    "print('expected target: ', training_data[0][1])\n",
    "\n",
    "tag_scores = tag_scores.detach().numpy()\n",
    "tag = [idx_to_tag[idx] for idx in np.argmax(tag_scores, axis = 1)]\n",
    "print('generated target: ', tag)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected target:  ['DET', 'NN', 'V', 'DET', 'NN']\n",
      "generated target:  ['DET', 'NN', 'V', 'DET', 'NN']\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ]
}
